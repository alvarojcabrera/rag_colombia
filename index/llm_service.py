from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.language_models import BaseChatModel
from typing import List, Dict, Any
from dotenv import load_dotenv
import os

class LLMService:
    """
    Servicio MEJORADO para generar respuestas usando Ollama + LLM.
    
    MEJORAS IMPLEMENTADAS:
    - Usa mÃ¡s chunks relevantes (hasta 10 en lugar de 5)
    - Filtra mejor los chunks por similarity score
    - No menciona "Fragmento X" en las respuestas
    - Mejor contexto y respuestas mÃ¡s naturales
    """

    llm: BaseChatModel
    
    def __init__(self, model_name: str = "gemma3n:e2b"):
        load_dotenv()
        openrouter_api_key = os.getenv("OPENROUTER_API_KEY")
        print(f"ğŸ¤– Inicializando LLM mejorado: {model_name}")
        if openrouter_api_key is None:
            print("API Key de OpenRouter no encontrada. Usando modelo local")
            self.llm = self.get_local_llm(model=model_name)
        else:
            print("API Key de OpenRouter encontrada. Usando modelo hosteado")
            self.llm = self.get_hosted_llm(openrouter_api_key)
        print("âœ… LLM mejorado inicializado correctamente")

    def get_local_llm(self, model_name: str = "gemma3n:e2b"):
        return ChatOllama(model=model_name)
    
    def get_hosted_llm(self, openrouter_api_key):
        # Usa un modelo hosteado en OpenRouter, compatible con la api de openai
        return ChatOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=openrouter_api_key,
            model="google/gemma-3n-e4b-it:free"
        )
    
    def filter_relevant_chunks(self, search_results: List[Dict[str, Any]], min_score: float = 0.25) -> List[Dict[str, Any]]:
        """
        Filtra chunks por similarity score y los ordena por relevancia.
        
        Args:
            search_results: Resultados de bÃºsqueda semÃ¡ntica
            min_score: Score mÃ­nimo para considerar relevante (0.25 = mÃ¡s inclusivo)
        """
        if not search_results:
            return []
        
        # Filtrar por score mÃ­nimo
        relevant_chunks = [
            chunk for chunk in search_results 
            if chunk.get('similarity_score', 0) >= min_score
        ]
        
        # Ordenar por score descendente (ya deberÃ­an estar ordenados, pero por seguridad)
        relevant_chunks.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        print(f"ğŸ“Š Chunks filtrados: {len(relevant_chunks)}/{len(search_results)} sobre threshold {min_score}")
        
        return relevant_chunks
    
    def create_enhanced_context(self, relevant_chunks: List[Dict[str, Any]]) -> str:
        """
        Crea un contexto mejorado sin mencionar "Fragmento X".
        Combina informaciÃ³n relacionada de forma mÃ¡s natural.
        """
        if not relevant_chunks:
            return ""
        
        # Agrupar por headers similares si existen
        context_parts = []
        
        for i, chunk in enumerate(relevant_chunks):
            content = chunk.get('content', '').strip()
            
            # Limpiar contenido
            if content:
                # Limitar tamaÃ±o pero no cortar bruscamente
                if len(content) > 800:
                    # Buscar un punto para cortar naturalmente
                    truncated = content[:800]
                    last_period = truncated.rfind('.')
                    last_space = truncated.rfind(' ')
                    
                    if last_period > 600:  # Si hay un punto cerca del final
                        content = content[:last_period + 1]
                    elif last_space > 600:  # Si no, cortar en espacio
                        content = content[:last_space] + "..."
                    else:
                        content = truncated + "..."
                
                context_parts.append(content)
        
        # Unir todo el contexto de forma natural
        context = "\n\n".join(context_parts)
        
        print(f"ğŸ“ Contexto creado con {len(context_parts)} secciones relevantes")
        return context
    
    def create_enhanced_prompt(self, query: str, context: str) -> str:
        """
        Crea un prompt mejorado que genera respuestas mÃ¡s naturales.
        """
        prompt = f"""Eres un experto en Colombia con acceso a informaciÃ³n actualizada de Wikipedia. Tu trabajo es responder preguntas sobre Colombia de manera clara, precisa y natural.

INSTRUCCIONES IMPORTANTES:
1. SOLO responde preguntas relacionadas con Colombia
2. Usa ÃšNICAMENTE la informaciÃ³n proporcionada abajo
3. Responde de forma natural y conversacional (NO menciones "fragmentos" ni "segÃºn el fragmento X")
4. Si la pregunta no es sobre Colombia, responde: "Solo puedo responder preguntas sobre Colombia. Â¿Hay algo especÃ­fico sobre Colombia que te gustarÃ­a saber?"
5. Si no hay informaciÃ³n suficiente, responde: "No tengo informaciÃ³n suficiente para responder esa pregunta especÃ­fica sobre Colombia."
6. SÃ© preciso pero amigable en tus respuestas

INFORMACIÃ“N DISPONIBLE SOBRE COLOMBIA:
{context}

PREGUNTA: {query}

RESPUESTA (natural y directa):"""
        
        return prompt
    
    def generate_answer(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """
        Genera una respuesta MEJORADA basada en la query y mÃ¡s chunks relevantes.
        
        MEJORAS:
        - Usa mÃ¡s chunks (hasta 10 en lugar de 5)
        - Mejor filtrado por similarity score
        - Respuestas mÃ¡s naturales sin mencionar "fragmentos"
        """
        try:
            # Si no hay resultados de bÃºsqueda
            if not search_results:
                return "No encontrÃ© informaciÃ³n relevante sobre Colombia para responder tu pregunta. Â¿PodrÃ­as ser mÃ¡s especÃ­fico?"
            
            # Verificar calidad general de los resultados
            best_score = max(r.get('similarity_score', 0) for r in search_results)
            
            print(f"ğŸ” Analizando {len(search_results)} chunks encontrados")
            print(f"ğŸ“Š Mejor score de similitud: {best_score:.3f}")
            
            # Si el mejor score es muy bajo, probablemente no es sobre Colombia
            if best_score < 0.3:
                return "Tu pregunta no parece estar relacionada con la informaciÃ³n que tengo sobre Colombia. Solo puedo responder preguntas sobre Colombia basÃ¡ndome en Wikipedia."
            
            # Filtrar chunks relevantes con threshold mÃ¡s bajo para incluir mÃ¡s informaciÃ³n
            relevant_chunks = self.filter_relevant_chunks(search_results, min_score=0.25)
            
            if not relevant_chunks:
                return "No encontrÃ© informaciÃ³n suficientemente relevante sobre Colombia para responder tu pregunta especÃ­fica."
            
            # Crear contexto mejorado
            context = self.create_enhanced_context(relevant_chunks)
            
            if not context.strip():
                return "No pude extraer informaciÃ³n Ãºtil para responder tu pregunta sobre Colombia."
            
            # Crear prompt mejorado
            prompt = self.create_enhanced_prompt(query, context)
            
            print(f"ğŸ§  Generando respuesta con {len(relevant_chunks)} chunks relevantes...")
            print(f"ğŸ“ TamaÃ±o del contexto: {len(context)} caracteres")
            
            # Generar respuesta
            response = str(self.llm.invoke(prompt).content)
            answer = response.strip()
            
            # ValidaciÃ³n adicional de la respuesta
            if not answer or len(answer) < 20:
                return "No pude generar una respuesta apropiada. Por favor, reformula tu pregunta sobre Colombia."
            
            print("âœ… Respuesta mejorada generada exitosamente")
            return answer
            
        except Exception as e:
            print(f"âŒ Error generando respuesta: {e}")
            return "Lo siento, hubo un error procesando tu pregunta sobre Colombia. Por favor intenta de nuevo."
    
    def test_simple(self) -> str:
        """
        MÃ©todo para probar que el LLM funciona bÃ¡sicamente.
        """
        try:
            response = self.llm.invoke("Di 'Hola' en una palabra").content
            return response.strip()
        except Exception as e:
            return f"Error: {str(e)}"
    
    def ask_question(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """
        MÃ©todo principal para hacer preguntas al sistema RAG.
        Utiliza la nueva lÃ³gica mejorada.
        """
        return self.generate_answer(query, search_results)