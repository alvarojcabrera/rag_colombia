from langchain_ollama import OllamaLLM
from typing import List, Dict, Any

class LLMService:
    """
    Servicio para generar respuestas usando Ollama + LLM.
    Toma los chunks encontrados por la b√∫squeda sem√°ntica y genera respuestas coherentes.
    SOLO responde preguntas sobre Colombia bas√°ndose en informaci√≥n de Wikipedia.
    """
    
    def __init__(self, model_name: str = "llama3.1:8b"):
        print(f"ü§ñ Inicializando LLM: {model_name}")
        self.llm = OllamaLLM(model=model_name)
        print("‚úÖ LLM inicializado correctamente")
    
    def create_prompt(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        """
        Crea el prompt combinando la pregunta del usuario con el contexto encontrado.
        Incluye validaciones estrictas para solo responder sobre Colombia.
        """
        # Construir el contexto con los chunks m√°s relevantes
        context_parts = []
        for i, chunk in enumerate(chunks[:3]):  # Solo los 3 m√°s relevantes
            content = chunk['content'][:500]  # Limitar tama√±o
            context_parts.append(f"Fragmento {i+1}: {content}")
        
        context = "\n\n".join(context_parts)
        
        # Crear el prompt con validaciones estrictas
        prompt = f"""Eres un asistente especializado √öNICAMENTE en Colombia. Tu base de datos contiene SOLAMENTE informaci√≥n de Wikipedia sobre Colombia.

REGLAS ESTRICTAS QUE DEBES SEGUIR:
1. SOLO responde preguntas relacionadas con Colombia (geograf√≠a, historia, cultura, pol√≠tica, econom√≠a, etc.)
2. SOLO usa la informaci√≥n proporcionada en los fragmentos de Wikipedia Colombia
3. Si la pregunta NO parece estar relacionada con Colombia, responde exactamente: "Solo puedo responder preguntas sobre Colombia. ¬øHay algo espec√≠fico sobre Colombia que te gustar√≠a saber?"
4. Si la pregunta ES sobre Colombia pero NO hay informaci√≥n suficiente en los fragmentos, responde: "No tengo informaci√≥n suficiente en mi base de datos sobre Colombia para responder esa pregunta espec√≠fica."
5. NUNCA inventes informaci√≥n que no est√© en los fragmentos proporcionados
6. Responde en espa√±ol de forma clara y concisa

INFORMACI√ìN DISPONIBLE DE COLOMBIA:
{context}

PREGUNTA DEL USUARIO: {query}

AN√ÅLISIS Y RESPUESTA:"""
        
        return prompt
    
    def generate_answer(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """
        Genera una respuesta basada en la query y los resultados de b√∫squeda.
        Incluye validaciones inteligentes basadas en similarity scores.
        """
        # Si no hay resultados de b√∫squeda
        if not search_results:
            return "No encontr√© informaci√≥n relevante sobre Colombia para responder tu pregunta. ¬øPodr√≠as ser m√°s espec√≠fico?"
        
        # Verificar la calidad de los resultados bas√°ndose en similarity scores
        best_score = max(r.get('similarity_score', 0) for r in search_results)
        
        # Si el mejor score es muy bajo, probablemente no es sobre Colombia
        if best_score < 0.4:
            return "Tu pregunta no parece estar relacionada con la informaci√≥n que tengo sobre Colombia. Solo puedo responder preguntas sobre Colombia bas√°ndome en Wikipedia."
        
        # Filtrar solo resultados con score decente
        relevant_results = [r for r in search_results if r.get('similarity_score', 0) > 0.3]
        
        if not relevant_results:
            return "No encontr√© informaci√≥n suficientemente relevante sobre Colombia para responder tu pregunta espec√≠fica."
        
        # Crear el prompt mejorado
        prompt = self.create_prompt(query, relevant_results)
        
        print(f"üß† Generando respuesta para: '{query[:50]}...'")
        print(f"üìä Mejor score de similitud: {best_score:.3f}")
        
        try:
            # Generar respuesta
            response = self.llm.invoke(prompt)
            print("‚úÖ Respuesta generada exitosamente")
            return response.strip()
            
        except Exception as e:
            print(f"‚ùå Error generando respuesta: {e}")
            return "Lo siento, hubo un error procesando tu pregunta sobre Colombia. Por favor intenta de nuevo."
    
    def test_simple(self) -> str:
        """
        M√©todo para probar que el LLM funciona b√°sicamente.
        """
        try:
            response = self.llm.invoke("Di 'Hola' en una palabra")
            return response.strip()
        except Exception as e:
            return f"Error: {str(e)}"
    
    def ask_question(self, query: str, search_results: List[Dict[str, Any]]) -> str:
        """
        M√©todo principal para hacer preguntas al sistema RAG.
        Utiliza la validaci√≥n inteligente basada en similarity scores.
        """
        return self.generate_answer(query, search_results)