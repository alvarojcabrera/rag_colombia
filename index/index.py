from index.extractor import Extractor
from index.splitter import Splitter
from index.embeddings import EmbeddingService
from index.vector_store import VectorStoreService
from index.llm_service import LLMService

class IndexService:

    def __init__(self):
        self.extractor = Extractor()
        self.splitter = Splitter()
        self.embedding_service = EmbeddingService()
        self.vector_store_service = VectorStoreService()
        self.llm_service = LLMService()
        
        # Inicializar conexiÃ³n a Pinecone para bÃºsquedas
        print("ğŸ”Œ Conectando a Pinecone para bÃºsquedas...")
        if self.vector_store_service.create_index_if_not_exists():
            print("âœ… ConexiÃ³n a Pinecone establecida")
        else:
            print("âŒ Error conectando a Pinecone")

    def index_pipeline(self):
        """
        Pipeline completo de indexaciÃ³n:
        1. ExtracciÃ³n y limpieza
        2. Splitting y chunking  
        3. GeneraciÃ³n de embeddings
        4. IndexaciÃ³n en base de datos vectorizada (Pinecone)
        """
        print("=== INICIANDO PIPELINE DE INDEXACIÃ“N ===\n")
        
        # Paso 1: ExtracciÃ³n
        print("PASO 1: ExtracciÃ³n y limpieza de contenido")
        url = "https://es.wikipedia.org/wiki/Colombia"
        markdown = self.extractor.extract_md(url)
        print("âœ… ExtracciÃ³n completada\n")
        
        # Paso 2: Splitting y chunking
        print("PASO 2: Splitting y chunking")
        chunks = self.splitter.split_md(markdown)
        print("âœ… Splitting completado\n")
        
        # Paso 3: GeneraciÃ³n de embeddings
        print("PASO 3: GeneraciÃ³n de embeddings")
        embedded_chunks = self.embedding_service.embed_chunks(chunks)
        print("âœ… Embeddings completados\n")
        
        # Paso 4: IndexaciÃ³n en Pinecone
        print("PASO 4: IndexaciÃ³n en base de datos vectorizada")
        
        # Crear Ã­ndice si no existe
        if self.vector_store_service.create_index_if_not_exists():
            # Indexar chunks
            if self.vector_store_service.index_chunks(embedded_chunks):
                print("âœ… IndexaciÃ³n en Pinecone completada\n")
            else:
                print("âŒ Error en indexaciÃ³n de Pinecone\n")
                return embedded_chunks  # Retornar chunks aunque falle Pinecone
        else:
            print("âŒ Error creando Ã­ndice de Pinecone\n")
            return embedded_chunks  # Retornar chunks aunque falle Pinecone
        
        print("=== PIPELINE COMPLETADO ===")
        print(f"Total de chunks procesados: {len(embedded_chunks)}")
        print(f"DimensiÃ³n de embeddings: {self.embedding_service.get_embedding_dimension()}")
        
        # Mostrar estadÃ­sticas de Pinecone
        stats = self.vector_store_service.get_index_stats()
        if 'error' not in stats:
            print(f"Vectores en Pinecone: {stats['total_vectors']}")
        
        return embedded_chunks
    
    def search(self, query: str, top_k: int = 5):
        """
        BÃºsqueda semÃ¡ntica en los chunks indexados usando Pinecone.
        """
        print(f"ğŸ” Realizando bÃºsqueda semÃ¡ntica: '{query}'")
        
        # Buscar en Pinecone
        results = self.vector_store_service.search_similar(query, top_k)
        
        if results:
            print(f"\nğŸ“‹ RESULTADOS DE BÃšSQUEDA:")
            for i, result in enumerate(results):
                print(f"\n   Resultado {i+1}:")
                print(f"   Score: {result['similarity_score']:.4f}")
                print(f"   Chunk ID: {result['chunk_id']}")
                print(f"   Contenido: {result['content'][:200]}...")
                if result['metadata']:
                    headers = {k: v for k, v in result['metadata'].items() if k.startswith('header_')}
                    if headers:
                        print(f"   Headers: {headers}")
        else:
            print("âŒ No se encontraron resultados")
        
        return results

    def ask_question(self, question: str) -> str:
        """
        MÃ©todo principal del RAG: pregunta â†’ bÃºsqueda â†’ generaciÃ³n â†’ respuesta
        """
        print(f"\nâ“ PREGUNTA: {question}")
        print("=" * 50)
        
        # 1. BÃºsqueda semÃ¡ntica (ya funciona)
        search_results = self.vector_store_service.search_similar(question, top_k=5)
        
        # 2. GeneraciÃ³n de respuesta con LLM (nuevo)
        answer = self.llm_service.generate_answer(question, search_results)
        
        print(f"\nğŸ’¡ RESPUESTA:")
        print(answer)
        print("=" * 50)
        
        return answer